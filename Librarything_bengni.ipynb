{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae742945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import dateutil.parser as parser\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861cb6f",
   "metadata": {},
   "source": [
    "#### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae14150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "### Evaluation metrics:\n",
    "def MSE(Y1, Y2):\n",
    "    return np.mean((Y1-Y2)**2)\n",
    "\n",
    "def binary_error_rate(Ypred, Ytest):\n",
    "    # Check binary error rate, see report section 2\n",
    "    TP = sum( np.logical_and(Ypred>=4.0, Ytest>=4.0) )\n",
    "    FP = sum( np.logical_and(Ypred>=4.0, Ytest<4.0) )\n",
    "    TN = sum( np.logical_and(Ypred<4.0, Ytest<4.0) )\n",
    "    FN = sum( np.logical_and(Ypred<4.0, Ytest>=4.0) )\n",
    "\n",
    "    assert TP+FP+TN+FN == len(Ytest)\n",
    "\n",
    "    Accuracy = (TP + TN) / len(Ytest)\n",
    "    BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    print(f\"TP:{TP}, FP:{FP}, TN:{TN}, FN:{FN}\")\n",
    "    print(f\"Accuracy:{Accuracy}, BER:{BER}\")\n",
    "    \n",
    "    \n",
    "def round_predictions(predictions):\n",
    "    '''\n",
    "    Round predictions to the nearest integer\n",
    "    '''\n",
    "    rounded_predictions = np.zeros_like(predictions)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        rounded_predictions[i] = int(round(pred))\n",
    "    return rounded_predictions\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16237984",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews with a rating: 1383597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'work': '3206242',\n",
       " 'flags': [],\n",
       " 'unixtime': 1194393600,\n",
       " 'stars': 5.0,\n",
       " 'nhelpful': 0,\n",
       " 'time': 'Nov 7, 2007',\n",
       " 'user': 'van_stef',\n",
       " 'length': 83}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review data\n",
    "data = []\n",
    "for d in parse(\"./lthing_data/reviews.json\"):\n",
    "    # filter review without rating\n",
    "    if ('stars' not in d) or d['stars'] == None: continue\n",
    "    # or without valid date\n",
    "    if ('unixtime' not in d) or (d['unixtime'] == None) or (d['unixtime'] == -86400): continue\n",
    "\n",
    "    # for this moment, we don't need actual text\n",
    "    d['length'] = len(d['comment']) # store the length\n",
    "    del d['comment']\n",
    "    \n",
    "    # train and test\n",
    "    data.append(d)\n",
    "\n",
    "print(f\"Total number of reviews with a rating: {len(data)}\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a7dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8:2 train-test split\n",
    "cut = int(len(data) * 0.8)\n",
    "train_data, test_data = data[:cut], data[cut:]\n",
    "del data # save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439e8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# social network data\n",
    "with open('lthing_data/edges.txt') as f:\n",
    "    edges = f.readlines()\n",
    "    \n",
    "friendsPerUser = defaultdict(set)\n",
    "for pair in edges:\n",
    "    friends = pair.strip().split()\n",
    "    friendsPerUser[friends[0]].add(friends[1])\n",
    "    friendsPerUser[friends[1]].add(friends[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b131ec",
   "metadata": {},
   "source": [
    "### Populate useful data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "602cc34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7653503816851766 3.9605122606260252\n"
     ]
    }
   ],
   "source": [
    "# for Jaccard similarity\n",
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "# incorporate the timestamp of the review\n",
    "usersTimePerItem = defaultdict(list) \n",
    "itemsTimePerUser = defaultdict(list)\n",
    "\n",
    "# very likely to have same ratings, so use list\n",
    "ratingsPerItem = defaultdict(list)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "\n",
    "# Each User/Item has a list of length 3: nhelpful, #abuse, #not_a_review\n",
    "recordPerUser = defaultdict(lambda:[0,0,0])\n",
    "recordPerItem = defaultdict(lambda:[0,0,0])\n",
    "\n",
    "for d in train_data:\n",
    "    \n",
    "    u, i, r, dt, dl = d['user'], d['work'], d['stars'], parser.parse(d['time']), d['length']\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "    usersTimePerItem[i].append( (dt, u, dl) )\n",
    "    itemsTimePerUser[u].append( (dt, i, dl) )\n",
    "    ratingsPerItem[i].append(r)\n",
    "    ratingsPerUser[u].append(r)\n",
    "    \n",
    "    if d['nhelpful']:\n",
    "        recordPerUser[u][0] += d['nhelpful']\n",
    "        recordPerItem[i][0] += d['nhelpful']\n",
    "    if d['flags']:\n",
    "        if 'abuse' in d['flags']:\n",
    "            recordPerUser[u][1] += 1\n",
    "            recordPerItem[i][1] += 1\n",
    "        if 'not_a_review' in d['flags']:\n",
    "            recordPerUser[u][2] += 1\n",
    "            recordPerItem[i][2] += 1\n",
    "\n",
    "# calculate 2 global averages for cold-start user or book: average of each user's/book's average ratings\n",
    "avgBookRating = sum( sum(ratingsPerItem[i])/len(ratingsPerItem[i]) for i in ratingsPerItem)/len(ratingsPerItem)\n",
    "avgUserRating = sum( sum(ratingsPerUser[u])/len(ratingsPerUser[u]) for u in ratingsPerUser)/len(ratingsPerUser)\n",
    "\n",
    "print(avgBookRating, avgUserRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e51013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 26049 users out of 65546 have helpful votes, abuse review, or not-a-review.\n",
      "A total of 54560 items out of 333400 have helpful votes, abuse review, or not-a-review.\n"
     ]
    }
   ],
   "source": [
    "print(f\"A total of {len(recordPerUser)} users out of {len(itemsPerUser)} have helpful votes, abuse review, or not-a-review.\")\n",
    "print(f\"A total of {len(recordPerItem)} items out of {len(usersPerItem)} have helpful votes, abuse review, or not-a-review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed54b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort interaction data structures by time for later use\n",
    "for i in usersTimePerItem:\n",
    "    usersTimePerItem[i].sort()\n",
    "    \n",
    "for u in itemsTimePerUser:\n",
    "    itemsTimePerUser[u].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119e762",
   "metadata": {},
   "source": [
    "# baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b5b8a",
   "metadata": {},
   "source": [
    "### A relevant simple baseline that predicts rating based on the average rating given by a user, and the average rating received by a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1278631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_baseline(d):\n",
    "    global avgBookRating, avgUserRating\n",
    "    result = [1] # bias term\n",
    "    u, i = d['user'], d['work']\n",
    "    if u in ratingsPerUser:\n",
    "        result.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    else:\n",
    "        result.append(avgUserRating)\n",
    "    if i in ratingsPerItem:\n",
    "        result.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    else:\n",
    "        result.append(avgBookRating)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da479360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_baseline = np.array( [feature_baseline(d) for d in train_data] )\n",
    "Xtest_baseline = np.array( [feature_baseline(d) for d in test_data] )\n",
    "\n",
    "Ytrain = np.array( [d['stars'] for d in train_data] )\n",
    "Ytest = np.array( [d['stars'] for d in test_data] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda878ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.87617111,  0.6602512 ,  0.83293227])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_baseline = linear_model.Ridge(1, fit_intercept=False)\n",
    "model_baseline.fit(Xtrain_baseline, Ytrain)\n",
    "\n",
    "model_baseline.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43352f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred baseline train:\n",
      "MSE =  0.49414739601287017\n",
      "TP:394096, FP:36270, TN:381454, FN:295057\n",
      "Accuracy:0.7006650242077485, BER:0.2574860347308826\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on train set\n",
    "Ypred_baseline_train = model_baseline.predict(Xtrain_baseline)\n",
    "print(\"Ypred baseline train:\")\n",
    "print(\"MSE = \", MSE(Ypred_baseline_train, Ytrain))\n",
    "binary_error_rate(Ypred_baseline_train, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2354c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred baseline train:\n",
      "MSE =  0.5692389488624301\n",
      "TP:623479, FP:167585, TN:250139, FN:65674\n",
      "Accuracy:0.7892638477446003, BER:0.24824132040848057\n"
     ]
    }
   ],
   "source": [
    "# round to nearest integers\n",
    "rounded_Ypred_baseline_train = round_predictions(Ypred_baseline_train)\n",
    "print(\"Rounded Ypred baseline train:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_baseline_train, Ytrain))\n",
    "binary_error_rate(rounded_Ypred_baseline_train, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4b4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred baseline test:\n",
      "MSE =  0.8487029094124011\n",
      "TP:79748, FP:17570, TN:86385, FN:93017\n",
      "Accuracy:0.6003649898814686, BER:0.3537086573763284\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "Ypred_baseline_test = model_baseline.predict(Xtest_baseline)\n",
    "print(\"Ypred baseline test:\")\n",
    "print(\"MSE = \", MSE(Ypred_baseline_test, Ytest))\n",
    "binary_error_rate(Ypred_baseline_test, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd39c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred baseline test:\n",
      "MSE =  0.9395164787510841\n",
      "TP:147292, FP:62277, TN:41678, FN:25473\n",
      "Accuracy:0.6828924544666088, BER:0.3732597909927997\n"
     ]
    }
   ],
   "source": [
    "# round Ypred to nearest integers\n",
    "rounded_Ypred_baseline_test = round_predictions(Ypred_baseline_test)\n",
    "print(\"Rounded Ypred baseline test:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_baseline_test, Ytest))\n",
    "binary_error_rate(rounded_Ypred_baseline_test, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5c100f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trivial baseline:\n",
      "MSE =  1.0329249783174328\n",
      "TP:172765, FP:103955, TN:0, FN:0\n",
      "Accuracy:0.6243314541775079, BER:0.5\n"
     ]
    }
   ],
   "source": [
    "# Compare it to a even more trivial baseline: always predict median 4.0\n",
    "trivial_baseline = np.array([4.0]*len(Ytest))\n",
    "print(\"Trivial baseline:\")\n",
    "print(\"MSE = \", MSE(trivial_baseline, Ytest))\n",
    "binary_error_rate(trivial_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "834e3d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trivial baseline:\n",
      "MSE =  1.038931606673551\n",
      "TP:689153, FP:417724, TN:0, FN:0\n",
      "Accuracy:0.6226102809977984, BER:0.5\n"
     ]
    }
   ],
   "source": [
    "trivial_baseline_train = np.array([4.0]*len(Ytrain))\n",
    "print(\"Trivial baseline:\")\n",
    "print(\"MSE = \", MSE(trivial_baseline_train, Ytrain))\n",
    "binary_error_rate(trivial_baseline_train, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eea04",
   "metadata": {},
   "source": [
    "### Basic feature engineering design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23115a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a user \"\"\"\n",
    "def featureU(u, t):\n",
    "    global avgUserRating\n",
    "    if u not in ratingsPerUser:\n",
    "        # cold-start user, see below\n",
    "        return [1, avgUserRating, 0,0,0,0]\n",
    "    \n",
    "    f = [1] # add a bias term\n",
    "    # average rating this user gives; average of all users' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    \n",
    "    # This user's number of 'not_a_review' and 'abuse' comments respectively; 0, 0\n",
    "    # Let the model decide their effect on rating prediction\n",
    "    f += recordPerUser[u]\n",
    "    # nhelpful received; 0\n",
    "    \n",
    "    # ??? time (maybe in month?) since his last reading; \n",
    "    \n",
    "    # number of books he has read till this time t; 0\n",
    "    # General opinion (rating habit may change as one read more books)\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    while c<len(itemsTimePerUser[u]) and dt > itemsTimePerUser[u][c][0]: \n",
    "        c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    assert(len(f) == 6)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a60e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2.5427350427350426, 31, 0, 5, 10],\n",
       " [1, 3.274193548387097, 9, 0, 0, 5],\n",
       " [1, 3.5917874396135265, 173, 0, 0, 116],\n",
       " [1, 3.9342105263157894, 13, 0, 1, 23],\n",
       " [1, 4.31875, 1, 0, 1, 74]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureU\n",
    "[featureU(d['user'], d['time']) for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8b29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a book \"\"\"\n",
    "def featureI(i, t):\n",
    "    global avgBookRating\n",
    "    if i not in ratingsPerItem:\n",
    "        # cold-start book\n",
    "        return [avgBookRating, 0,0,0,0]\n",
    "    \n",
    "    f = []\n",
    "    # average rating this book receives; average of all books' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    \n",
    "    # This item's nhelpful vote\n",
    "    f.append(recordPerItem[i][0])\n",
    "    # nhelpful received; 0\n",
    "    \n",
    "    # length of time interval (in month) this book was read by people (t_last_read - t_first_read); 0\n",
    "    diff_times = usersTimePerItem[i][-1][0] - usersTimePerItem[i][0][0]\n",
    "    f.append( diff_times.days / 30 )\n",
    "    \n",
    "    # average length of the comment it received; 0\n",
    "    all_lengths = [user[2] for user in usersTimePerItem[i]]\n",
    "    f.append( sum(all_lengths) / len(all_lengths) )\n",
    "    \n",
    "    # number of users that have read this book till this time t; 0\n",
    "    # General opinion (rating may change as a book is read more times)\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    while c<len(usersTimePerItem[i]) and dt > usersTimePerItem[i][c][0]: \n",
    "        c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    assert(len(f) == 5)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "567442b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 0, 0.0, 294.0, 1],\n",
       " [4.625, 0, 64.33333333333333, 766.25, 0],\n",
       " [4.484795321637427, 625, 62.56666666666667, 890.1280701754386, 616],\n",
       " [3.9272727272727272, 3, 26.9, 1122.3636363636363, 31],\n",
       " [3.8990384615384617, 25, 24.466666666666665, 1121.6153846153845, 95]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureI\n",
    "[featureI(d['work'], d['time']) for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c2171b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to user-item interactions \"\"\"\n",
    "def featureInter(u, i):\n",
    "    f = []\n",
    "    # Book/item similarity (max similarity between i and the books that u has read)\n",
    "    book_max_sim = 0\n",
    "    if u in itemsPerUser:\n",
    "        for book in itemsPerUser[u]:\n",
    "            if book == i: continue\n",
    "            sim = Jaccard(usersPerItem[i], usersPerItem[book])\n",
    "            if sim > book_max_sim:\n",
    "                book_max_sim = sim\n",
    "    f.append(book_max_sim)\n",
    "    \n",
    "    # User similarity (max similarity between u and the users that have read i)\n",
    "    user_max_sim = 0\n",
    "    if i in usersPerItem:\n",
    "        for user in usersPerItem[i]:\n",
    "            if user == u: continue\n",
    "            sim = Jaccard(itemsPerUser[u], itemsPerUser[user])\n",
    "            if sim > user_max_sim:\n",
    "                user_max_sim = sim\n",
    "    f.append(user_max_sim)\n",
    "    \n",
    "    # Number of u’s friends that have read i\n",
    "    num_friends = 0\n",
    "    if u in friendsPerUser:\n",
    "        for friend in friendsPerUser[u]:\n",
    "            if friend in usersPerItem[i]:\n",
    "                num_friends += 1\n",
    "    f.append(num_friends)\n",
    "    \n",
    "    # Average rating u’s friends gives\n",
    "    sum_friends_rating = 0\n",
    "    num_friends_rating = 0\n",
    "    if u in friendsPerUser:\n",
    "        for friend in friendsPerUser[u]:\n",
    "            if friend in ratingsPerUser:\n",
    "                sum_friends_rating += sum(ratingsPerUser[friend])\n",
    "                num_friends_rating += len(ratingsPerUser[friend])\n",
    "    avg_friends_rating = 0\n",
    "    if num_friends_rating == 0:\n",
    "        f.append(0)\n",
    "    else:\n",
    "        f.append(sum_friends_rating / num_friends_rating)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f281d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0024875621890547263, 0.0043859649122807015, 0, 3.8947784810126582],\n",
       " [0.01098901098901099, 0.0032258064516129032, 0, 0],\n",
       " [0.08033240997229917, 0.04234527687296417, 0, 3.839939024390244],\n",
       " [0.0375, 0.02654867256637168, 0, 3.9117161716171616],\n",
       " [0.04591836734693878, 0.0379746835443038, 0, 3.8333333333333335]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureInter\n",
    "[featureInter(d['user'], d['work']) for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42bf34",
   "metadata": {},
   "source": [
    "### Test new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a04f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_featureUI = np.array( [featureU(d['user'], d['time']) + featureI(d['work'], d['time']) + featureInter(d['user'], d['work']) for d in train_data] )\n",
    "Xtest_featureUI = np.array( [featureU(d['user'], d['time']) + featureI(d['work'], d['time']) + featureInter(d['user'], d['work']) for d in test_data] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "da2883ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.87930251e+00,  6.60209984e-01, -1.24935129e-05,  2.09952691e-04,\n",
       "       -6.47237432e-04, -2.13446597e-04,  8.33901735e-01,  7.61556072e-06,\n",
       "        1.40845145e-05, -1.31423936e-05, -1.67225930e-03,  2.85758755e-02,\n",
       "        4.90432332e-03])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_featureUI = linear_model.Ridge(1, fit_intercept=False)\n",
    "model_featureUI.fit(Xtrain_featureUI, Ytrain)\n",
    "\n",
    "model_featureUI.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c57bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred featureUI train:\n",
      "MSE =  0.49344140592319824\n",
      "TP:395455, FP:36656, TN:381068, FN:293698\n",
      "Accuracy:0.7015440740028025, BER:0.25696206933898225\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on train set\n",
    "Ypred_featureUI_train = model_featureUI.predict(Xtrain_featureUI)\n",
    "print(\"Ypred featureUI train:\")\n",
    "print(\"MSE = \", MSE(Ypred_featureUI_train, Ytrain))\n",
    "binary_error_rate(Ypred_featureUI_train, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "173093d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred featureUI train:\n",
      "MSE =  0.5682496790519633\n",
      "TP:623279, FP:167287, TN:250437, FN:65874\n",
      "Accuracy:0.7893523851340303, BER:0.24802973121602023\n"
     ]
    }
   ],
   "source": [
    "# round to nearest integers\n",
    "rounded_Ypred_featureUI_train = round_predictions(Ypred_featureUI_train)\n",
    "print(\"Rounded Ypred featureUI train:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_featureUI_train, Ytrain))\n",
    "binary_error_rate(rounded_Ypred_featureUI_train, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d229f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred featureUI test:\n",
      "MSE =  0.8487489794325093\n",
      "TP:80113, FP:17676, TN:86279, FN:92652\n",
      "Accuracy:0.6013009540329575, BER:0.35316214514437483\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "Ypred_featureUI_test = model_featureUI.predict(Xtest_featureUI)\n",
    "print(\"Ypred featureUI test:\")\n",
    "print(\"MSE = \", MSE(Ypred_featureUI_test, Ytest))\n",
    "binary_error_rate(Ypred_featureUI_test, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f909bfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred featureUI test:\n",
      "MSE =  0.9396574154379879\n",
      "TP:147235, FP:62392, TN:41563, FN:25530\n",
      "Accuracy:0.6822708875397514, BER:0.3739778789090582\n"
     ]
    }
   ],
   "source": [
    "# round to nearest integers\n",
    "rounded_Ypred_featureUI_test = round_predictions(Ypred_featureUI_test)\n",
    "print(\"Rounded Ypred featureUI test:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_featureUI_test, Ytest))\n",
    "binary_error_rate(rounded_Ypred_featureUI_test, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2cbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd97db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
