{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae742945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import dateutil.parser as parser\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861cb6f",
   "metadata": {},
   "source": [
    "#### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae14150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "### Evaluation metrics:\n",
    "def MSE(Y1, Y2):\n",
    "    return np.mean((Y1-Y2)**2)\n",
    "\n",
    "def binary_error_rate(Ypred, Ytest):\n",
    "    # Check binary error rate, see report section 2\n",
    "    TP = sum( np.logical_and(Ypred>=4.0, Ytest>=4.0) )\n",
    "    FP = sum( np.logical_and(Ypred>=4.0, Ytest<4.0) )\n",
    "    TN = sum( np.logical_and(Ypred<4.0, Ytest<4.0) )\n",
    "    FN = sum( np.logical_and(Ypred<4.0, Ytest>=4.0) )\n",
    "\n",
    "    assert TP+FP+TN+FN == len(Ytest)\n",
    "\n",
    "    Accuracy = (TP + TN) / len(Ytest)\n",
    "    BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    print(f\"TP:{TP}, FP:{FP}, TN:{TN}, FN:{FN}\")\n",
    "    print(f\"Accuracy:{Accuracy}, BER:{BER}\")\n",
    "    \n",
    "    \n",
    "def round_predictions(predictions):\n",
    "    '''\n",
    "    Round predictions to the nearest integer\n",
    "    '''\n",
    "    rounded_predictions = np.zeros_like(predictions)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        rounded_predictions[i] = int(round(pred))\n",
    "    return rounded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16237984",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews with a rating: 1387125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'work': '3206242',\n",
       " 'flags': [],\n",
       " 'unixtime': 1194393600,\n",
       " 'stars': 5.0,\n",
       " 'nhelpful': 0,\n",
       " 'time': 'Nov 7, 2007',\n",
       " 'user': 'van_stef',\n",
       " 'length': 83}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for d in parse(\"./lthing_data/reviews.json\"):\n",
    "    # filter review without rating\n",
    "    if 'stars' not in d: continue\n",
    "    # There are also 90 reviews with no date\n",
    "    if not d['time']: continue\n",
    "\n",
    "    # for this moment, we don't need actual text\n",
    "    d['length'] = len(d['comment']) # store the length\n",
    "    del d['comment']\n",
    "    \n",
    "    # train and test\n",
    "    data.append(d)\n",
    "\n",
    "print(f\"Total number of reviews with a rating: {len(data)}\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a7dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8:2 train-test split\n",
    "cut = int(len(data) * 0.8)\n",
    "train_data, test_data = data[:cut], data[cut:]\n",
    "del data # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b131ec",
   "metadata": {},
   "source": [
    "### Populate useful data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "602cc34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7654177390812986 3.5\n"
     ]
    }
   ],
   "source": [
    "usersPerItem = defaultdict(list)\n",
    "itemsPerUser = defaultdict(list)\n",
    "\n",
    "# very likely to have same ratings, so use list\n",
    "ratingsPerItem = defaultdict(list)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "\n",
    "# Each User has a list of length 3: nhelpful, #abuse, #not_a_review\n",
    "recordPerUser = defaultdict(lambda:[0,0,0])\n",
    "\n",
    "for d in train_data:\n",
    "    \n",
    "    u, i, r, dt, dl = d['user'], d['work'], d['stars'], parser.parse(d['time']), d['length']\n",
    "    usersPerItem[i].append( (dt, u, dl) )\n",
    "    itemsPerUser[u].append( (dt, i, dl) )\n",
    "    ratingsPerItem[i].append(r)\n",
    "    ratingsPerUser[u].append(r)\n",
    "    \n",
    "    if d['nhelpful']:\n",
    "        recordPerUser[u][0] += d['nhelpful']\n",
    "    if d['flags']:\n",
    "        if 'abuse' in d['flags']:\n",
    "            recordPerUser[u][1] += 1\n",
    "        if 'not_a_review' in d['flags']:\n",
    "            recordPerUser[u][2] += 1\n",
    "\n",
    "# calculate 2 global averages for cold-start user or book: average of each user's/book's average ratings\n",
    "avgBookRating = sum( sum(ratingsPerItem[i])/len(ratingsPerItem[i]) for i in ratingsPerItem)/len(ratingsPerItem)\n",
    "avgUserRating = sum( sum(ratingsPerUser[u])/len(ratingsPerUser[u]) for i in ratingsPerUser)/len(ratingsPerUser)\n",
    "\n",
    "print(avgBookRating, avgUserRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e51013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 26105 users out of 65768 have helpful votes, abuse review, or not-a-review\n"
     ]
    }
   ],
   "source": [
    "print(f\"A total of {len(recordPerUser)} users out of {len(itemsPerUser)} have helpful votes, abuse review, or not-a-review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119e762",
   "metadata": {},
   "source": [
    "# baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b5b8a",
   "metadata": {},
   "source": [
    "### A relevant simple baseline that predicts rating based on the average rating given by a user, and the average rating received by a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1278631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_baseline(d):\n",
    "    global avgBookRating, avgUserRating\n",
    "    result = [1] # bias term\n",
    "    u, i = d['user'], d['work']\n",
    "    if u in ratingsPerUser:\n",
    "        result.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    else:\n",
    "        result.append(avgUserRating)\n",
    "    if i in ratingsPerItem:\n",
    "        result.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    else:\n",
    "        result.append(avgBookRating)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da479360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_baseline = np.array( [feature_baseline(d) for d in train_data] )\n",
    "Xtest_baseline = np.array( [feature_baseline(d) for d in test_data] )\n",
    "\n",
    "Ytrain = np.array( [d['stars'] for d in train_data] )\n",
    "Ytest = np.array( [d['stars'] for d in test_data] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cda878ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.04929319, 3.68309618, 3.59532967, ..., 3.66405694, 3.55441755,\n",
       "       4.53077834])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_baseline = linear_model.LinearRegression()\n",
    "model_baseline.fit(Xtrain_baseline, Ytrain)\n",
    "\n",
    "Ypred_baseline = model_baseline.predict(Xtest_baseline)\n",
    "Ypred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c4b4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred baseline:\n",
      "MSE =  0.8515584703996876\n",
      "TP:78705, FP:17295, TN:86892, FN:94533\n",
      "Accuracy:0.5969072722357394, BER:0.35584120736810165\n"
     ]
    }
   ],
   "source": [
    "print(\"Ypred baseline:\")\n",
    "print(\"MSE = \", MSE(Ypred_baseline, Ytest))\n",
    "binary_error_rate(Ypred_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecd39c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred baseline:\n",
      "MSE =  0.9418248175182482\n",
      "TP:147137, FP:62126, TN:42061, FN:26101\n",
      "Accuracy:0.6819789132197891, BER:0.3734793809657283\n"
     ]
    }
   ],
   "source": [
    "# round Ypred to nearest integers\n",
    "rounded_Ypred_baseline = round_predictions(Ypred_baseline)\n",
    "print(\"Rounded Ypred baseline:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_baseline, Ytest))\n",
    "binary_error_rate(rounded_Ypred_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5c100f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trivial baseline:\n",
      "MSE =  1.0327899432278995\n",
      "TP:173238, FP:104187, TN:0, FN:0\n",
      "Accuracy:0.6244498513111651, BER:0.5\n"
     ]
    }
   ],
   "source": [
    "# Compare it to a even more trivial baseline: always predict median 4.0\n",
    "trivial_baseline = np.array([4.0]*len(Ytest))\n",
    "print(\"Trivial baseline:\")\n",
    "print(\"MSE = \", MSE(trivial_baseline, Ytest))\n",
    "binary_error_rate(trivial_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eea04",
   "metadata": {},
   "source": [
    "### Basic feature engineering design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "23115a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a user \"\"\"\n",
    "def featureU(u, t):\n",
    "    global avgUserRating\n",
    "    if u not in ratingsPerUser:\n",
    "        # cold-start user, see below\n",
    "        return [0, avgUserRating, 0,0,0, 0]\n",
    "    \n",
    "    f = []\n",
    "    # How many books this user have read; if cold-start, 0.\n",
    "    f.append( len(itemsPerUser[u]) )\n",
    "    # average rating this user gives; average of all users' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    \n",
    "    # This user's number of 'not_a_review' and 'abuse' comments respectively; 0, 0\n",
    "    # Let the model decide their effect on rating prediction\n",
    "    f += recordPerUser[u]\n",
    "    # nhelpful received; 0\n",
    "    \n",
    "    # ??? time (maybe in month?) since his last reading; \n",
    "    \n",
    "    # number of books he has read till this time t; 0\n",
    "    # General opinion (rating habit may change as one read more books)\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    for item in itemsPerUser[u]:\n",
    "        if item[0] >= dt:\n",
    "            c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "49a60e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[59, 3.940677966101695, 4, 0, 0, 38],\n",
       " [2, 3.5, 0, 0, 0, 0],\n",
       " [20, 3.85, 2, 0, 0, 3],\n",
       " [138, 3.6340579710144927, 8, 0, 0, 21],\n",
       " [19, 3.6842105263157894, 7, 4, 4, 0]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureU\n",
    "[featureU(d['user'], 'Apr 7, 2011') for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7a8b29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a book \"\"\"\n",
    "def featureI(i, t):\n",
    "    global avgBookRating\n",
    "    if i not in ratingsPerItem:\n",
    "        # cold-start book\n",
    "        return [0, avgBookRating, 0, 0, 0]\n",
    "    \n",
    "    f = []\n",
    "    # How many users have read this book; if cold-start, 0.\n",
    "    f.append( len(usersPerItem[i]) )\n",
    "    \n",
    "    # average rating this book receives; average of all books' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    \n",
    "    # length of time interval (in month) this book was read by people (t_last_read - t_first_read); 0\n",
    "    all_times = [user[0] for user in usersPerItem[i]]\n",
    "    f.append( (max(all_times) - min(all_times)).days / 30 )\n",
    "    \n",
    "    # average length of the comment it received; 0\n",
    "    all_lengths = [user[2] for user in usersPerItem[i]]\n",
    "    f.append( sum(all_lengths) / len(all_lengths) )\n",
    "    \n",
    "    # number of users who have read this book till time t; 0\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    for user in usersPerItem[i]:\n",
    "        if user[0] >= dt:\n",
    "            c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "567442b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52, 3.9903846153846154, 74.0, 778.2884615384615, 18],\n",
       " [10, 3.9, 58.1, 299.5, 4],\n",
       " [29, 3.5172413793103448, 79.06666666666666, 766.551724137931, 13],\n",
       " [16, 3.1875, 56.733333333333334, 864.875, 1],\n",
       " [7, 4.357142857142857, 64.0, 445.14285714285717, 2]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureI\n",
    "[featureI(d['work'], 'Apr 7, 2011') for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10680a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04f862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
