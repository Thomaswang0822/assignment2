{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae742945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea3859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodo anehan\n",
      "\n",
      "Rodo sevilemar\n",
      "\n",
      "Rodo dingsi\n",
      "\n",
      "Rodo slash\n",
      "\n",
      "RelaxedReader AnnRig\n",
      "\n",
      "RelaxedReader bookbroke\n",
      "\n",
      "RelaxedReader Bumpersmom\n",
      "\n",
      "RelaxedReader DivaColumbus\n",
      "\n",
      "RelaxedReader AnnRig\n",
      "\n",
      "RelaxedReader bookbroke\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219790"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"./lthing_data/edges.txt\", 'r')\n",
    "count = 0\n",
    "for line in f:\n",
    "    if count < 10:\n",
    "        print(line)\n",
    "    count += 1\n",
    "\n",
    "f.close()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f936ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews with a rating: 1387125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'work': '12198649',\n",
       " 'flags': [],\n",
       " 'unixtime': 1333756800,\n",
       " 'stars': 5.0,\n",
       " 'nhelpful': 0,\n",
       " 'time': 'Apr 7, 2012',\n",
       " 'user': 'dwatson2',\n",
       " 'length': 2582}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "data = []\n",
    "\n",
    "for d in parse(\"./lthing_data/reviews.json\"):\n",
    "    # filter review without rating\n",
    "    if 'stars' not in d: continue\n",
    "    # There are also 90 reviews with no date\n",
    "    if not d['time']: continue\n",
    "\n",
    "    # for this moment, we don't need actual text\n",
    "    d['length'] = len(d['comment'])\n",
    "    del d['comment']\n",
    "    \n",
    "    # train and test\n",
    "    data.append(d)\n",
    "    count += 1\n",
    "\n",
    "print(f\"Total number of reviews with a rating: {count}\")\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95df4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8:2 train-test split\n",
    "cut = int(count * 0.8)\n",
    "train_data, test_data = data[:cut], data[cut:]\n",
    "del data # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2dc01d",
   "metadata": {},
   "source": [
    "### Populate useful data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c86d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7654177390812986 3.5\n"
     ]
    }
   ],
   "source": [
    "usersPerItem = defaultdict(list)\n",
    "itemsPerUser = defaultdict(list)\n",
    "\n",
    "# very likely to have same ratings, so use list\n",
    "ratingsPerItem = defaultdict(list)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "\n",
    "# Each User has a list of length 3: nhelpful, #abuse, #not_a_review\n",
    "recordPerUser = defaultdict(lambda:[0,0,0])\n",
    "\n",
    "for d in train_data:\n",
    "    \n",
    "    u, i, r, dt = d['user'], d['work'], d['stars'], parser.parse(d['time'])\n",
    "    usersPerItem[i].append( (dt, u) )\n",
    "    itemsPerUser[u].append( (dt, i) )\n",
    "    ratingsPerItem[i].append(r)\n",
    "    ratingsPerUser[u].append(r)\n",
    "    \n",
    "    if d['nhelpful']:\n",
    "        recordPerUser[u][0] += d['nhelpful']\n",
    "    if d['flags']:\n",
    "        if 'abuse' in d['flags']:\n",
    "            recordPerUser[u][1] += 1\n",
    "        if 'not_a_review' in d['flags']:\n",
    "            recordPerUser[u][2] += 1\n",
    "\n",
    "# calculate 2 global averages for cold-start user or book: average of each user's/book's average ratings\n",
    "avgBookRating = sum( sum(ratingsPerItem[i])/len(ratingsPerItem[i]) for i in ratingsPerItem)/len(ratingsPerItem)\n",
    "avgUserRating = sum( sum(ratingsPerUser[u])/len(ratingsPerUser[u]) for i in ratingsPerUser)/len(ratingsPerUser)\n",
    "\n",
    "print(avgBookRating, avgUserRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72274a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 26105 users out of 65768 have helpful votes, abuse review, or not-a-review\n"
     ]
    }
   ],
   "source": [
    "print(f\"A total of {len(recordPerUser)} users out of {len(itemsPerUser)} have helpful votes, abuse review, or not-a-review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93a946",
   "metadata": {},
   "source": [
    "### Sort interaction data structures by time for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665fcbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in usersPerItem:\n",
    "    usersPerItem[i].sort()\n",
    "    \n",
    "for u in itemsPerUser:\n",
    "    itemsPerUser[u].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119e762",
   "metadata": {},
   "source": [
    "# baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd90a0",
   "metadata": {},
   "source": [
    "### A relevant simple baseline that predicts rating based on the average rating given by a user, and the average rating received by a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3eb399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_baseline(d):\n",
    "    global avgBookRating, avgUserRating\n",
    "    result = [1] # bias term\n",
    "    u, i = d['user'], d['work']\n",
    "    if u in ratingsPerUser:\n",
    "        result.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    else:\n",
    "        result.append(avgUserRating)\n",
    "    if i in ratingsPerItem:\n",
    "        result.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    else:\n",
    "        result.append(avgBookRating)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_baseline = np.array( [feature_baseline(d) for d in train_data] )\n",
    "Xtest_baseline = np.array( [feature_baseline(d) for d in test_data] )\n",
    "\n",
    "Ytrain = np.array( [d['stars'] for d in train_data] )\n",
    "Ytest = np.array( [d['stars'] for d in test_data] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df55ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = linear_model.LinearRegression()\n",
    "model_baseline.fit(Xtrain_baseline, Ytrain)\n",
    "\n",
    "Ypred_baseline = model_baseline.predict(Xtest_baseline)\n",
    "Ypred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(Y1, Y2):\n",
    "    return np.mean( (Y1-Y2)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa950cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MSE\n",
    "MSE(Ypred_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47821d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare it to a even more trivial baseline: always predict median 4.0\n",
    "MSE([4.0]*len(Ytest), Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87de043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check binary error rate, see report section 2\n",
    "TP = sum( np.logical_and(Ypred_baseline>=4.0, Ytest>=4.0) )\n",
    "FP = sum( np.logical_and(Ypred_baseline>=4.0, Ytest<4.0) )\n",
    "TN = sum( np.logical_and(Ypred_baseline<4.0, Ytest<4.0) )\n",
    "FN = sum( np.logical_and(Ypred_baseline<4.0, Ytest>=4.0) )\n",
    "\n",
    "assert TP+FP+TN+FN == len(Ytest)\n",
    "\n",
    "Accuracy = (TP + TN) / len(Ytest)\n",
    "BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "print(f\"TP:{TN}, FP:{FP}, TN:{TN}, FN:{FN}\")\n",
    "print(f\"Accuracy:{Accuracy}, BER:{BER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb83c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7961296",
   "metadata": {},
   "source": [
    "## Basic feature engineering design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b481fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a user \"\"\"\n",
    "def featureU(u, t):\n",
    "    global avgUserRating\n",
    "    if u not in ratingsPerUser:\n",
    "        # cold-start user, see below\n",
    "        return [0, avgUserRating, 0,0,0, 0]\n",
    "    \n",
    "    f = []\n",
    "    # How many books this user have read; if cold-start, 0.\n",
    "    f.append( len(itemsPerUser[u]) )\n",
    "    # average rating this user gives; average of all users' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    \n",
    "    # This user's number of 'not_a_review' and 'abuse' comments respectively; 0, 0\n",
    "    # Let the model decide their effect on rating prediction\n",
    "    f += recordPerUser[u]\n",
    "    # nhelpful received; 0\n",
    "    \n",
    "    # ??? time (maybe in month?) since his last reading; \n",
    "    \n",
    "    # number of books he has read till this time t; 0\n",
    "    # General opinion (rating habit may change as one read more books)\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    while c<len(itemsPerUser[u]) and dt <= itemsPerUser[u][c][0]: \n",
    "        c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4580b99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[59, 3.940677966101695, 4, 0, 0, 0],\n",
       " [2, 3.5, 0, 0, 0, 0],\n",
       " [20, 3.85, 2, 0, 0, 0],\n",
       " [138, 3.6340579710144927, 8, 0, 0, 0],\n",
       " [19, 3.6842105263157894, 7, 4, 4, 0],\n",
       " [111, 4.045045045045045, 0, 0, 0, 111],\n",
       " [12, 3.5, 0, 0, 0, 12],\n",
       " [705, 3.248936170212766, 369, 1, 0, 0],\n",
       " [2, 4.0, 0, 0, 0, 0],\n",
       " [55, 4.1454545454545455, 0, 0, 3, 0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test our function\n",
    "[featureU(d['user'], 'Apr 7, 2012') for d in test_data][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfae623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureI(i, t):\n",
    "    # How many users have read this book; 0\n",
    "    \n",
    "    # average rating this book receives; average of all books' average ratings (see baseline)\n",
    "    \n",
    "    # length of time interval (in month) this book was read by people (t_last_read - t_first_read); 0\n",
    "    \n",
    "    # average length of the comment it received; 0\n",
    "    \n",
    "    # number of users who have read this book till time t; 0\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef121a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureInter(u, i):\n",
    "    # old-school book similarity (is this book similar to what I have read?); 0\n",
    "    \n",
    "    # AFTER make use of social networks: how often was this book read by the user's friends?\n",
    "    # cold-start book:0; cold-start user: (total read/total user)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031793fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for d in parse(\"./lthing_data/reviews.json\"):\n",
    "    if 'stars' not in d: continue\n",
    "    if d['flags']:\n",
    "        print(d)\n",
    "        c += 1\n",
    "    if c==5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c649d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
