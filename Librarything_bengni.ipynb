{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae742945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import dateutil.parser as parser\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861cb6f",
   "metadata": {},
   "source": [
    "#### Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae14150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "### Evaluation metrics:\n",
    "def MSE(Y1, Y2):\n",
    "    return np.mean((Y1-Y2)**2)\n",
    "\n",
    "def binary_error_rate(Ypred, Ytest):\n",
    "    # Check binary error rate, see report section 2\n",
    "    TP = sum( np.logical_and(Ypred>=4.0, Ytest>=4.0) )\n",
    "    FP = sum( np.logical_and(Ypred>=4.0, Ytest<4.0) )\n",
    "    TN = sum( np.logical_and(Ypred<4.0, Ytest<4.0) )\n",
    "    FN = sum( np.logical_and(Ypred<4.0, Ytest>=4.0) )\n",
    "\n",
    "    assert TP+FP+TN+FN == len(Ytest)\n",
    "\n",
    "    Accuracy = (TP + TN) / len(Ytest)\n",
    "    BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    print(f\"TP:{TP}, FP:{FP}, TN:{TN}, FN:{FN}\")\n",
    "    print(f\"Accuracy:{Accuracy}, BER:{BER}\")\n",
    "    \n",
    "    \n",
    "def round_predictions(predictions):\n",
    "    '''\n",
    "    Round predictions to the nearest integer\n",
    "    '''\n",
    "    rounded_predictions = np.zeros_like(predictions)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        rounded_predictions[i] = int(round(pred))\n",
    "    return rounded_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16237984",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews with a rating: 1387125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'work': '3206242',\n",
       " 'flags': [],\n",
       " 'unixtime': 1194393600,\n",
       " 'stars': 5.0,\n",
       " 'nhelpful': 0,\n",
       " 'time': 'Nov 7, 2007',\n",
       " 'user': 'van_stef',\n",
       " 'length': 83}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for d in parse(\"./lthing_data/reviews.json\"):\n",
    "    # filter review without rating\n",
    "    if 'stars' not in d: continue\n",
    "    # There are also 90 reviews with no date\n",
    "    if not d['time']: continue\n",
    "\n",
    "    # for this moment, we don't need actual text\n",
    "    d['length'] = len(d['comment']) # store the length\n",
    "    del d['comment']\n",
    "    \n",
    "    # train and test\n",
    "    data.append(d)\n",
    "\n",
    "print(f\"Total number of reviews with a rating: {len(data)}\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a7dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8:2 train-test split\n",
    "cut = int(len(data) * 0.8)\n",
    "train_data, test_data = data[:cut], data[cut:]\n",
    "del data # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b131ec",
   "metadata": {},
   "source": [
    "### Populate useful data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602cc34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7654177390812986 3.5\n"
     ]
    }
   ],
   "source": [
    "usersPerItem = defaultdict(list)\n",
    "itemsPerUser = defaultdict(list)\n",
    "\n",
    "# very likely to have same ratings, so use list\n",
    "ratingsPerItem = defaultdict(list)\n",
    "ratingsPerUser = defaultdict(list)\n",
    "\n",
    "# Each User/Item has a list of length 3: nhelpful, #abuse, #not_a_review\n",
    "recordPerUser = defaultdict(lambda:[0,0,0])\n",
    "recordPerItem = defaultdict(lambda:[0,0,0])\n",
    "\n",
    "for d in train_data:\n",
    "    \n",
    "    u, i, r, dt, dl = d['user'], d['work'], d['stars'], parser.parse(d['time']), d['length']\n",
    "    usersPerItem[i].append( (dt, u, dl) )\n",
    "    itemsPerUser[u].append( (dt, i, dl) )\n",
    "    ratingsPerItem[i].append(r)\n",
    "    ratingsPerUser[u].append(r)\n",
    "    \n",
    "    if d['nhelpful']:\n",
    "        recordPerUser[u][0] += d['nhelpful']\n",
    "        recordPerItem[i][0] += d['nhelpful']\n",
    "    if d['flags']:\n",
    "        if 'abuse' in d['flags']:\n",
    "            recordPerUser[u][1] += 1\n",
    "            recordPerItem[i][1] += 1\n",
    "        if 'not_a_review' in d['flags']:\n",
    "            recordPerUser[u][2] += 1\n",
    "            recordPerItem[i][2] += 1\n",
    "\n",
    "# calculate 2 global averages for cold-start user or book: average of each user's/book's average ratings\n",
    "avgBookRating = sum( sum(ratingsPerItem[i])/len(ratingsPerItem[i]) for i in ratingsPerItem)/len(ratingsPerItem)\n",
    "avgUserRating = sum( sum(ratingsPerUser[u])/len(ratingsPerUser[u]) for i in ratingsPerUser)/len(ratingsPerUser)\n",
    "\n",
    "print(avgBookRating, avgUserRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e51013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 26105 users out of 65768 have helpful votes, abuse review, or not-a-review.\n",
      "A total of 54692 items out of 334336 have helpful votes, abuse review, or not-a-review.\n"
     ]
    }
   ],
   "source": [
    "print(f\"A total of {len(recordPerUser)} users out of {len(itemsPerUser)} have helpful votes, abuse review, or not-a-review.\")\n",
    "print(f\"A total of {len(recordPerItem)} items out of {len(usersPerItem)} have helpful votes, abuse review, or not-a-review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd838aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort interaction data structures by time for later use\n",
    "for i in usersPerItem:\n",
    "    usersPerItem[i].sort()\n",
    "    \n",
    "for u in itemsPerUser:\n",
    "    itemsPerUser[u].sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119e762",
   "metadata": {},
   "source": [
    "# baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b5b8a",
   "metadata": {},
   "source": [
    "### A relevant simple baseline that predicts rating based on the average rating given by a user, and the average rating received by a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1278631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_baseline(d):\n",
    "    global avgBookRating, avgUserRating\n",
    "    result = [1] # bias term\n",
    "    u, i = d['user'], d['work']\n",
    "    if u in ratingsPerUser:\n",
    "        result.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    else:\n",
    "        result.append(avgUserRating)\n",
    "    if i in ratingsPerItem:\n",
    "        result.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    else:\n",
    "        result.append(avgBookRating)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da479360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_baseline = np.array( [feature_baseline(d) for d in train_data] )\n",
    "Xtest_baseline = np.array( [feature_baseline(d) for d in test_data] )\n",
    "\n",
    "Ytrain = np.array( [d['stars'] for d in train_data] )\n",
    "Ytest = np.array( [d['stars'] for d in test_data] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cda878ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.04929319, 3.68309618, 3.59532967, ..., 3.66405694, 3.55441755,\n",
       "       4.53077834])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_baseline = linear_model.LinearRegression()\n",
    "model_baseline.fit(Xtrain_baseline, Ytrain)\n",
    "\n",
    "Ypred_baseline = model_baseline.predict(Xtest_baseline)\n",
    "Ypred_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4b4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred baseline:\n",
      "MSE =  0.8515584703996876\n",
      "TP:78705, FP:17295, TN:86892, FN:94533\n",
      "Accuracy:0.5969072722357394, BER:0.35584120736810165\n"
     ]
    }
   ],
   "source": [
    "print(\"Ypred baseline:\")\n",
    "print(\"MSE = \", MSE(Ypred_baseline, Ytest))\n",
    "binary_error_rate(Ypred_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd39c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred baseline:\n",
      "MSE =  0.9418248175182482\n",
      "TP:147137, FP:62126, TN:42061, FN:26101\n",
      "Accuracy:0.6819789132197891, BER:0.3734793809657283\n"
     ]
    }
   ],
   "source": [
    "# round Ypred to nearest integers\n",
    "rounded_Ypred_baseline = round_predictions(Ypred_baseline)\n",
    "print(\"Rounded Ypred baseline:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_baseline, Ytest))\n",
    "binary_error_rate(rounded_Ypred_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c100f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trivial baseline:\n",
      "MSE =  1.0327899432278995\n",
      "TP:173238, FP:104187, TN:0, FN:0\n",
      "Accuracy:0.6244498513111651, BER:0.5\n"
     ]
    }
   ],
   "source": [
    "# Compare it to a even more trivial baseline: always predict median 4.0\n",
    "trivial_baseline = np.array([4.0]*len(Ytest))\n",
    "print(\"Trivial baseline:\")\n",
    "print(\"MSE = \", MSE(trivial_baseline, Ytest))\n",
    "binary_error_rate(trivial_baseline, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eea04",
   "metadata": {},
   "source": [
    "### Basic feature engineering design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23115a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a user \"\"\"\n",
    "def featureU(u, t):\n",
    "    global avgUserRating\n",
    "    if u not in ratingsPerUser:\n",
    "        # cold-start user, see below\n",
    "        return [1, 0, avgUserRating, 0,0,0]\n",
    "    \n",
    "    f = [1] # add a bias term\n",
    "    # How many books this user have read; if cold-start, 0.\n",
    "    f.append( len(itemsPerUser[u]) )\n",
    "    # average rating this user gives; average of all users' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerUser[u]) / len(ratingsPerUser[u]) )\n",
    "    \n",
    "    # This user's number of 'nhelpful', 'not_a_review' and 'abuse' comments;\n",
    "    f.append(recordPerUser[u][0]) # nhelpful received; 0\n",
    "    f.append(recordPerUser[u][1]+recordPerUser[u][1]) # not_a_review + abuse; 0\n",
    "    \n",
    "    # ??? time (maybe in month?) since his last reading; \n",
    "    \n",
    "    # number of books he has read till this time t; 0\n",
    "    # General opinion (rating habit may change as one read more books)\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    while c<len(itemsPerUser[u]) and dt > itemsPerUser[u][c][0]: \n",
    "        c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49a60e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 59, 3.940677966101695, 4, 0, 3],\n",
       " [1, 2, 3.5, 0, 0, 0],\n",
       " [1, 20, 3.85, 2, 0, 14],\n",
       " [1, 138, 3.6340579710144927, 8, 0, 97],\n",
       " [1, 19, 3.6842105263157894, 7, 8, 18]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureU\n",
    "[featureU(d['user'], d['time']) for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a8b29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Integrate features relavent to a book \"\"\"\n",
    "def featureI(i, t):\n",
    "    global avgBookRating\n",
    "    if i not in ratingsPerItem:\n",
    "        # cold-start book\n",
    "        return [0, avgBookRating, 0,0,0,0,0]\n",
    "    \n",
    "    f = []\n",
    "    # How many users have read this book; if cold-start, 0.\n",
    "    f.append( len(usersPerItem[i]) )\n",
    "    \n",
    "    # average rating this book receives; average of all books' average ratings (see baseline)\n",
    "    f.append( sum(ratingsPerItem[i]) / len(ratingsPerItem[i]) )\n",
    "    \n",
    "    # This item's number of 'nhelpful', 'not_a_review' and 'abuse' comments;\n",
    "    f.append(recordPerItem[i][0]) # nhelpful received; 0\n",
    "    f.append(recordPerItem[i][1]+recordPerItem[i][1]) # not_a_review + abuse; 0\n",
    "    \n",
    "    # length of time interval (in month) this book was read by people (t_last_read - t_first_read); 0\n",
    "    all_times = [user[0] for user in usersPerItem[i]]\n",
    "    f.append( (max(all_times) - min(all_times)).days / 30 )\n",
    "    \n",
    "    # average length of the comment it received; 0\n",
    "    all_lengths = [user[2] for user in usersPerItem[i]]\n",
    "    f.append( sum(all_lengths) / len(all_lengths) )\n",
    "    \n",
    "    # number of users that have read this book till this time t; 0\n",
    "    # General opinion (rating may change as a book is read more times)\n",
    "    c = 0\n",
    "    dt = parser.parse(t)\n",
    "    while c<len(usersPerItem[i]) and dt > usersPerItem[i][c][0]: \n",
    "        c += 1\n",
    "    f.append(c)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "567442b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52, 3.9903846153846154, 12, 0, 74.0, 778.2884615384615, 31],\n",
       " [10, 3.9, 0, 0, 58.1, 299.5, 5],\n",
       " [29, 3.5172413793103448, 7, 2, 79.06666666666666, 766.551724137931, 10],\n",
       " [16, 3.1875, 5, 0, 56.733333333333334, 864.875, 14],\n",
       " [7, 4.357142857142857, 0, 0, 64.0, 445.14285714285717, 1]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test featureI\n",
    "[featureI(d['work'], d['time']) for d in test_data[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1309285",
   "metadata": {},
   "source": [
    "### Test new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a04f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_featureUI = np.array( [featureU(d['user'], d['time']) + featureI(d['work'], d['time']) for d in train_data] )\n",
    "Xtest_featureUI = np.array( [featureU(d['user'], d['time']) + featureI(d['work'], d['time']) for d in test_data] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e01adc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.04679692, 3.68239797, 3.5874592 , ..., 3.65084232, 3.54483989,\n",
       "       4.51515312])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_featureUI = linear_model.LinearRegression()\n",
    "model_featureUI.fit(Xtrain_featureUI, Ytrain)\n",
    "\n",
    "Ypred_featureUI = model_featureUI.predict(Xtest_featureUI)\n",
    "Ypred_featureUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "997e0ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  1.08806194e-05,  6.61156108e-01,  1.03536895e-07,\n",
       "        2.65666453e-05, -3.52203909e-07,  3.78305726e-04,  8.31807786e-01,\n",
       "       -5.34404513e-05, -4.60085434e-03,  6.49774129e-05, -9.24218845e-06,\n",
       "       -5.62146032e-04])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_featureUI.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "821f27a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ypred featureUI:\n",
      "MSE =  0.8503963587912803\n",
      "TP:78913, FP:17378, TN:86809, FN:94325\n",
      "Accuracy:0.5973578444624673, BER:0.355639199433999\n"
     ]
    }
   ],
   "source": [
    "print(\"Ypred featureUI:\")\n",
    "print(\"MSE = \", MSE(Ypred_featureUI, Ytest))\n",
    "binary_error_rate(Ypred_featureUI, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f2d1842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded Ypred featureUI:\n",
      "MSE =  0.9407037938181491\n",
      "TP:147085, FP:62105, TN:42082, FN:26153\n",
      "Accuracy:0.6818671713075606, BER:0.37352868318344834\n"
     ]
    }
   ],
   "source": [
    "# round Ypred to nearest integers\n",
    "rounded_Ypred_featureUI = round_predictions(Ypred_featureUI)\n",
    "print(\"Rounded Ypred featureUI:\")\n",
    "print(\"MSE = \", MSE(rounded_Ypred_featureUI, Ytest))\n",
    "binary_error_rate(rounded_Ypred_featureUI, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd4050f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
